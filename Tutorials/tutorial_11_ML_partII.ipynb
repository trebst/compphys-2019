{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border-style: none;\">\n",
    "<tr style=\"border-style: none\">\n",
    "<td style=\"border-style: none; width: 1%; font-size: 16px\">Institut f&uuml;r Theoretische Physik<br /> Universit&auml;t zu K&ouml;ln</td>\n",
    "<td style=\"border-style: none; width: 1%; font-size: 16px\">&nbsp;</td>\n",
    "<td style=\"border-style: none; width: 1%; text-align: right; font-size: 16px\">Prof. Dr. Simon Trebst<br /><b>Peter Br&ouml;cker</b></td>\n",
    "</tr>\n",
    "</table>\n",
    "<hr>\n",
    "<h1 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px;\">Computerphysik</h1>\n",
    "<h1 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px;\">Vorlesung &mdash; Programmiertechniken 11 / Part II\n",
    "</h1>\n",
    "<hr>\n",
    "<h3 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px; margin-bottom: 20px;\">Sommersemester 2019</h3>\n",
    "\n",
    "**Website:** [http://www.thp.uni-koeln.de/trebst/Lectures/2019-CompPhys.shtml](http://www.thp.uni-koeln.de/trebst/Lectures/2019-CompPhys.shtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II &mdash; Neuronales Netz mit Stochastic Gradient Descent und Mini Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir arbeiten nun mit dem sogenannten <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST Datensatz</a> handschriftlicher Ziffern, der standardmäßig dazu genutzt wird, um die Qualität verschiedener Designs von neuronalen Netzen zu überprüfen. Dazu laden wir ihn zunächst hinunter und untersuchen dann den Inhalt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]add DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]add CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using PyCall\n",
    "cm = pyimport(\"matplotlib.cm\")\n",
    "using PyPlot\n",
    "using LaTeXStrings\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST Datensatz</a> handschriftlicher Ziffern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die erste Spalte ist das Label, während der Rest eine digitalisierte, handschriftliche Ziffer der Größe $28 \\times 28$ darstellt. Wir separieren nun Label von Zahl und betrachten dann ein paar Beispielziffern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DOWNLOAD FILES FROM SERVER\n",
    "if !isfile(\"mnist_train.csv\")\n",
    "    download(\"http://www.thp.uni-koeln.de/trebst/Lectures/CompPhys-2019/mnist_train.csv\", \"mnist_train.csv\")\n",
    "end\n",
    "if !isfile(\"mnist_test.csv\")\n",
    "    download(\"http://www.thp.uni-koeln.de/trebst/Lectures/CompPhys-2019/mnist_test.csv\", \"mnist_test.csv\")\n",
    "end\n",
    "\n",
    "\n",
    "# READ RAW DATA\n",
    "test  = CSV.read(\"mnist_test.csv\",  header=false)\n",
    "train = CSV.read(\"mnist_train.csv\", header=false)\n",
    "\n",
    "# Ausgabe der RAW Daten Groesse\n",
    "println(\"Größe der Datensätze\")\n",
    "println(size(test))\n",
    "println(size(train))\n",
    "\n",
    "\n",
    "\n",
    "# generiere Einzellisten TEST SET\n",
    "test_inputs = convert(Matrix, (test[2:end])) ./ 255\n",
    "test_labels = zeros(Float64, size(test_inputs, 1), 10)\n",
    "# konvertiere i --> 0, 0, 0, ... 0, 1, 0 wobei 1 an Stelle i steht.\n",
    "for i in 1:size(test_inputs)[1]\n",
    "    test_labels[i,test[i, 1] + 1] = 1.0\n",
    "end\n",
    "\n",
    "# generiere Einzellisten TRAINING SET\n",
    "train_inputs = convert(Matrix, (train[2:end])) ./ 255\n",
    "train_labels = zeros(Float64, size(train_inputs, 1), 10)\n",
    "# konvertiere i --> 0, 0, 0, ... 0, 1, 0 wobei 1 an Stelle i steht.\n",
    "for i in 1:size(train_inputs)[1]\n",
    "    train_labels[i,train[i, 1] + 1] = 1.0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(10, 10))\n",
    "\n",
    "println(\"Beispiele Training-Set\")\n",
    "subplot(221)\n",
    "println(train_labels[1, :],\"   Ziffer = \", findmax(train_labels[1, :])[2]-1)\n",
    "imshow( transpose(reshape(train_inputs[1, :], 28, 28)), interpolation=\"none\", cmap=\"gist_gray\" )\n",
    "axis(\"off\")\n",
    "\n",
    "subplot(222)\n",
    "println(train_labels[2, :],\"   Ziffer = \", findmax(train_labels[2, :])[2]-1)\n",
    "imshow( transpose(reshape(train_inputs[2, :], 28, 28)), interpolation=\"none\", cmap=\"gist_gray\" )\n",
    "axis(\"off\")\n",
    "\n",
    "println(\"\\nBeispiele Test-Set\")\n",
    "subplot(223)\n",
    "println(test_labels[5, :],\"   Ziffer = \", findmax(test_labels[5, :])[2]-1)\n",
    "imshow( transpose(reshape(test_inputs[5, :], 28, 28)), interpolation=\"none\", cmap=\"gist_gray\" )\n",
    "axis(\"off\")\n",
    "\n",
    "subplot(224)\n",
    "println(test_labels[6, :],\"   Ziffer = \", findmax(test_labels[6, :])[2]-1)\n",
    "imshow( transpose(reshape(test_inputs[6, :], 28, 28)), interpolation=\"none\", cmap=\"gist_gray\" )\n",
    "axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anstatt nun alle Datensätze auf einmal zu verwenden, wie wir es in dem ersten, einfacheren Beispiel getan haben, verwenden wir nun einzelne Teilmengen, sogenannte **mini batches**, wobei es wichtig ist, dass diese Teilmengen groß genug sind, um die Gesamtmenge gut genug zu repräsentieren. Es ist wichtig vor jedem Durchlauf die Daten zu mischen, um zu verhindern, dass in den Daten Zyklen enstehen, in denen das Netzwerk hängen bleibt. Die Anzahl der Durchläufe, die mit den gesamten Daten passieren, wird in diesem Kontext als *epoch* bezeichnet. Wie viele man davon braucht, um eine Netzwerk ausreichend gut zu trainineren, hängt sehr stark vom Problem ab und muss empirisch bestimmt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(z)       = 1.0 / (1.0 + exp(-z));\n",
    "sigmoid_prime(z) = sigmoid(z) * (1.0 - sigmoid(z));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Netzwerk-Architektur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# INITIALIZATION BLOCK\n",
    "#----------------------\n",
    "\n",
    "# SET UP THE NEURAL NETWORK\n",
    "\n",
    "# how many neurons per layer are there\n",
    "n_input  = 28 * 28   # fixed by input\n",
    "n_hidden = 200       # can be set by user\n",
    "n_output = 10        # fixed by number of letters\n",
    "\n",
    "\n",
    "# the matrices describing the connections between layers\n",
    "W_input_hidden  = (rand(n_hidden, n_input)  .- 0.5) .* 2.0;\n",
    "W_hidden_output = (rand(n_output, n_hidden) .- 0.5) .* 2.0;\n",
    "\n",
    "# the bias on every layer\n",
    "B_hidden = (rand(n_hidden, 1) .- 0.5) .* 2.0;\n",
    "B_output = (rand(n_output, 1) .- 0.5) .* 2.0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Überwachtes Lernen** des neuronalen Netzes mit **Back Propagation**, **Stochastic Gradient Descent** und **Mini Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# LEARNING BLOCK\n",
    "#----------------------\n",
    "\n",
    "\n",
    "# PARAMETERS\n",
    "\n",
    "# Number of epochs of learning (i.e. iterations)\n",
    "epochs     = 10\n",
    "# size of every batch and number of batches\n",
    "batch_size = 250\n",
    "batches    = 20 #Int(floor(0.1 * size(train_inputs)[1] / batch_size))\n",
    "# Learning parameter, strength of learning effect\n",
    "gamma      = 0.01\n",
    "\n",
    "# An array with all indices of test data, for shuffeling and sorting into batches\n",
    "order_list = collect(1:size(train_inputs)[1])\n",
    "\n",
    "\n",
    "# LEARNING\n",
    "\n",
    "# iterate over all epochs\n",
    "for epoch in 1:epochs\n",
    "    \n",
    "    # gib die aktuelle epoche aus\n",
    "    println(\"Epoche \", epoch)\n",
    "        \n",
    "    # shuffle the list of indices so the first ones are random\n",
    "    shuffle!(order_list)\n",
    "        \n",
    "    # iterate over all batches\n",
    "    for b in 1:batches\n",
    "        \n",
    "        # initialize the errors of the layer quantities\n",
    "        dW_input_hidden  = zeros(size(W_input_hidden))\n",
    "        dW_hidden_output = zeros(size(W_hidden_output))\n",
    "        dB_hidden        = zeros(size(B_hidden))\n",
    "        dB_output        = zeros(size(B_output))\n",
    "        \n",
    "        # collect the indices for this batch\n",
    "        batch = order_list[1+(b-1)*batch_size : (b)*batch_size]\n",
    "        # iterate over all data within this batch\n",
    "        for i in batch\n",
    "            \n",
    "            # load the input data from the list\n",
    "            input_data  = train_inputs[i, :]\n",
    "            # load the target data, i.e. the expected output from the list\n",
    "            target_data = train_labels[i, :]\n",
    "            \n",
    "            # propagate the input through the net to get the output        \n",
    "            hidden_layer_values  =  sigmoid.( W_input_hidden * input_data + B_hidden )\n",
    "            output_layer_values  =  sigmoid.( W_hidden_output * hidden_layer_values + B_output )\n",
    "            \n",
    "            # start backpropagation and determine the error in the result as well as the resulting delta\n",
    "            result_err   = target_data - output_layer_values\n",
    "            result_delta = result_err .* sigmoid_prime.(output_layer_values)\n",
    "            \n",
    "            # go on backpropagating to the hidden layer and do the same\n",
    "            hidden_err   = W_hidden_output' * result_delta\n",
    "            hidden_delta = hidden_err .* sigmoid_prime.(hidden_layer_values)\n",
    "            \n",
    "            # increase the overall errors with the respective values\n",
    "            dW_input_hidden  += (hidden_delta * input_data') .* gamma\n",
    "            dW_hidden_output += (result_delta * hidden_layer_values') .* gamma\n",
    "            dB_hidden        += hidden_delta .* gamma\n",
    "            dB_output        += result_delta .* gamma\n",
    "        end\n",
    "        \n",
    "        # after the batch has been gone through, increment the weights and bias\n",
    "        W_input_hidden  += dW_input_hidden\n",
    "        W_hidden_output += dW_hidden_output\n",
    "        B_hidden        += dB_hidden\n",
    "        B_output        += dB_output\n",
    "        \n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anwendung** des optimierten Netzes auf den Test-Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# TESTING BLOCK (I)\n",
    "#----------------------\n",
    "\n",
    "# choose a random test index\n",
    "check = rand(1:size(test_inputs,1))\n",
    "\n",
    "# show the image\n",
    "imshow( transpose(reshape(test_inputs[check, :], 28, 28)), interpolation=\"none\", cmap=\"gist_gray\" )\n",
    "axis(\"off\")\n",
    "\n",
    "# print the expected result\n",
    "println(\"Expected result:  $( findmax(test_labels[check, :])[2]-1 )   ($(test_labels[check, :]))\")\n",
    "\n",
    "# propagate through the net and investigate output\n",
    "input_data  = test_inputs[check, :]\n",
    "hidden_layer_values  =  sigmoid.( W_input_hidden * input_data + B_hidden )\n",
    "output_layer_values  =  sigmoid.( W_hidden_output * hidden_layer_values + B_output )\n",
    "output = round.(output_layer_values, digits=3)\n",
    "println(\"Neural net gives: $(findmax(output[:,1])[2]-1)   ($(output[:,1]))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Berechnung der **Treffer-/Fehlerquote**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# TESTING BLOCK (II)\n",
    "#----------------------\n",
    "\n",
    "# test all configurations, 0 correct to begin with\n",
    "correct = 0\n",
    "\n",
    "# iterate over all indices\n",
    "for check in 1:size(test_inputs, 1)\n",
    "\n",
    "    # print the expected result\n",
    "    expected = findmax(test_labels[check, :])[2]-1\n",
    "\n",
    "    # propagate through the net and investigate output\n",
    "    input_data  = test_inputs[check, :]\n",
    "    hidden_layer_values  =  sigmoid.( W_input_hidden * input_data + B_hidden )\n",
    "    output_layer_values  =  sigmoid.( W_hidden_output * hidden_layer_values + B_output )\n",
    "    output = round.(output_layer_values, digits=3)\n",
    "    output_value = findmax(output[:,1])[2]-1\n",
    "    \n",
    "    # add to the list depending on correct or not\n",
    "    if output_value == expected\n",
    "        correct += 1\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "# print how many were correct\n",
    "println(\"$(correct) correct out of $(size(test_inputs,1))  ($(round(correct/size(test_inputs,1)*100, digits=2))%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
